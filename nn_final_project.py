# -*- coding: utf-8 -*-
"""nn_final_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zYVoVcZp7etNprq8qBdBwiP3qj97cz6F
"""

!pip install lime

import tensorflow as tf
from transformers import TFDistilBertModel, DistilBertTokenizer
import pandas as pd
from sklearn.model_selection import train_test_split
import os
from tensorflow.keras.models import Model
from IPython.display import HTML
from lime.lime_text import LimeTextExplainer
import numpy as np

def load_filtered_data():
    posts_df = pd.read_csv("/content/reddit_posts.csv")
    key_df = pd.read_csv("/content/key.csv")

    common_column = 'sm_id'
    merged_df = pd.merge(
        posts_df[[common_column, 'text']],
        key_df[[common_column, 'group']],
        on=common_column,
        how='inner'
    )

    filtered_df = merged_df[merged_df['group'].isin(['ed', 'diet'])]

    clean_df = filtered_df.dropna(subset=['text', 'group'])
    texts = clean_df['text'].tolist()
    labels = clean_df['group'].tolist()

    #binary labels (ED = 1, Diet = 0)
    binary_labels = [1 if label == "ed" else 0 for label in labels]

    return texts, binary_labels

class BERTDataGenerator(tf.keras.utils.Sequence):
    def __init__(self, texts, labels, tokenizer, batch_size=8, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.batch_size = batch_size
        self.max_length = max_length

    def __len__(self):
        return (len(self.texts) + self.batch_size - 1) // self.batch_size

    def __getitem__(self, index):
        start_idx = index * self.batch_size
        end_idx = min((index + 1) * self.batch_size, len(self.texts))

        batch_texts = self.texts[start_idx:end_idx]
        batch_labels = self.labels[start_idx:end_idx]

        encodings = self.tokenizer(
            batch_texts,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='tf'
        )

        batch_labels = tf.keras.utils.to_categorical(batch_labels, num_classes=2)

        return {
            'input_ids': encodings['input_ids'],
            'attention_mask': encodings['attention_mask']
        }, batch_labels

def create_bert_model():
    distilbert = TFDistilBertModel.from_pretrained('distilbert-base-uncased')

    input_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name='input_ids')
    attention_mask = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name='attention_mask')

    distilbert_output = distilbert(input_ids=input_ids, attention_mask=attention_mask)[0]

    #use [CLS] token (first token) for classification
    cls_output = distilbert_output[:, 0, :]

    dropout = tf.keras.layers.Dropout(0.1)(cls_output)
    output = tf.keras.layers.Dense(2, activation='softmax')(dropout)

    model = tf.keras.Model(
        inputs=[input_ids, attention_mask],
        outputs=output
    )

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

def train_bert_model(texts, labels, epochs=3, batch_size=8, save_dir='ed_model'):
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        texts, labels, test_size=0.2, random_state=42, stratify=labels
    )

    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

    train_generator = BERTDataGenerator(train_texts, train_labels, tokenizer, batch_size)
    val_generator = BERTDataGenerator(val_texts, val_labels, tokenizer, batch_size)

    model = create_bert_model()

    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            filepath=f'{save_dir}/best_model.h5',
            save_best_only=True,
            monitor='val_accuracy',
            mode='max'
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=2
        ),
        tf.keras.callbacks.TensorBoard(
            log_dir=f'{save_dir}/logs'
        )
    ]

    history = model.fit(
        train_generator,
        epochs=epochs,
        validation_data=val_generator,
        callbacks=callbacks
    )

    train_performance = model.evaluate(train_generator)
    val_performance = model.evaluate(val_generator)

    print(f"Training performance: {train_performance}")
    print(f"Validation performance: {val_performance}")

    os.makedirs(save_dir, exist_ok=True)
    model.save(f'{save_dir}/final_model')

    #save tokenizer and history
    tokenizer.save_pretrained(f'{save_dir}/tokenizer')

    import json
    with open(f'{save_dir}/training_history.json', 'w') as f:
        json.dump(history.history, f)

    return model, tokenizer, history

texts, labels = load_filtered_data()
model, tokenizer, history = train_bert_model(
    texts,
    labels,
    epochs=3,
    batch_size=8
)

import zipfile

zipfile.ZipFile("/content/ed_model.zip").extractall("/content")

saved_model = tf.saved_model.load('/content/ed_model/final_model')
saved_tokenizer = DistilBertTokenizer.from_pretrained('/content/ed_model/tokenizer')

serving_fn = model.signatures['serving_default']

def visualize_with_lime(text, model, tokenizer):
    def predict_proba(texts):
        results = []
        for t in texts:
            encodings = tokenizer(
                t,
                truncation=True,
                padding='max_length',
                max_length=512,
                return_tensors='tf'
            )

            outputs = serving_fn(
                input_ids=encodings['input_ids'],
                attention_mask=encodings['attention_mask']
            )

            output_key = list(outputs.keys())[0]
            probabilities = outputs[output_key].numpy()[0]

            results.append(probabilities)

        return np.array(results)

    explainer = LimeTextExplainer(class_names=['Diet', 'ED'])

    exp = explainer.explain_instance(
        text,
        predict_proba,
        num_features=10,
        num_samples=100
    )

    probs = predict_proba([text])[0]
    prediction = np.argmax(probs)
    confidence = probs[prediction]

    lime_html = exp.as_html()

    result = f"<p><strong>Prediction:</strong> {'ED' if prediction == 1 else 'Diet'} "
    result += f"(Confidence: {confidence:.2f})</p>"
    result += lime_html

    return HTML(result)

example_text = "I've been counting calories obsessively and weighing myself multiple times a day."
visualization = visualize_with_lime(example_text, saved_model, saved_tokenizer)
display(visualization)

!zip -r /content/ed_model.zip /content/ed_model